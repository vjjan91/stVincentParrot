[["index.html", "Source code and supporting information for Passive acoustic monitoring of the St. Vincent’s Parrot Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Source code and supporting information for Passive acoustic monitoring of the St. Vincent’s Parrot Vijay Ramesh Last compiled on 06 August, 2025 Section 1 Introduction This is the readable version that showcases analyses oof bioacoustic data for the St. Vincent’s parrot 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (repo maintainer) Postdoctoral Research Associate, Cornell Lab of Ornithology 1.2 Data access The data used in this work will be archived on Zenodo. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],["site-selection.html", "Section 2 Site selection 2.1 Load necessary libraries 2.2 Load study area and associated shapefiles 2.3 Extracting elevation from previous survey points 2.4 Interactive visualization of existing survey locations 2.5 Manually edit and choose new points for the study 2.6 Convert the new survey locations to a dataframe 2.7 Create shapefiles for fieldwork 2.8 Saving an interactive file for collaborators 2.9 Create .gpx file for GPS", " Section 2 Site selection The aim of this script is to identify locations that are suitable to carry out acoustic monitoring of the St. Vincent’s parrot across St. Vincent. Specific factors to consider are distance between recorders, elevation, habitat type and logistical constraints. This script was initially generated during a workshop with the Forestry Department of St. Vincent and the Grenadines on Nov 4th 2024 and sites were generated in real-time by consulting Forestry Staff. Note: Load the workspace saved (siteSelection.RData) if you have already used functions from mapEdit to interactively edit the map and add locations. 2.1 Load necessary libraries library(sf) library(tidyverse) library(mapview) library(mapedit) library(purrr) library(terra) library(spatstat.random) # for plotting library(viridis) library(colorspace) library(scales) library(ggplot2) library(patchwork) library(fastmap) 2.2 Load study area and associated shapefiles First, we will plot shapefiles locations where points were chosen for a previous exploratory study. We currently have locations for Forestry Watchpoints, Gap points and previous and potential locations where AudioMoth recorders were and could be deployed. # load watchPoints watchPoints &lt;- st_read(&quot;data/spatial/watchPoints.shp&quot;) # load gap points gapPoints &lt;- st_read(&quot;data/spatial/All_gaps.shp&quot;) gapPoints &lt;- st_zm(gapPoints) # load potential audioMoth locations # the above points also contain locations from a previous deployment of the AudioMoths potentialAM &lt;-st_read(&quot;data/spatial/potentialAM.shp&quot;) potentialAM &lt;- st_zm(potentialAM) potentialAM &lt;- st_transform(potentialAM, crs = 32620) # new study design # the below shapefile contains random points chosen for the new survey surveyDesign &lt;- st_read(&quot;data/spatial/_strRndSmp_trails_10m_stratum.shp&quot;) surveyDesign &lt;- st_transform(surveyDesign, crs = 32620) 2.3 Extracting elevation from previous survey points We will extract elevation from previous survey locations and edit formatting for the existing objects prior to editing the same. # create bounding box/polygon interactively for St.Vincent # for the sake of this exercise, we created a bounding box that is larger than the total area required for sampling # Run the following two lines if you haven&#39;t previously executed the same # bbox &lt;- vis %&gt;% # editMap() # bounding box for elevation bb_stVincent &lt;- st_bbox(bbox$finished, crs = 4326) %&gt;% st_as_sfc() %&gt;% st_sf() %&gt;% st_transform(., crs = 32620) # load elevation alt &lt;- terra::rast(&quot;data/elevation/alos-30m.tif&quot;) # note: the following line of code can take a few minutes for execution alt &lt;- terra::project(alt, &quot;epsg:32620&quot;) alt.hills &lt;- terra::crop(alt, bb_stVincent) rm(alt) # add elevation to each of the above shapefiles/points by extracting them from the digital elevation model elev_watch &lt;- terra::extract(alt.hills, watchPoints); names(elev_watch) &lt;- c(&quot;ID&quot;,&quot;elev&quot;) elev_gap &lt;- terra::extract(alt.hills, gapPoints) names(elev_gap) &lt;- c(&quot;ID&quot;, &quot;elev&quot;) elev_potentialAM &lt;- terra::extract(alt.hills, potentialAM) names(elev_potentialAM) &lt;- c(&quot;ID&quot;,&quot;elev&quot;) elev_surveyDesign &lt;- terra::extract(alt.hills, surveyDesign) names(elev_surveyDesign) &lt;- c(&quot;ID&quot;,&quot;elev&quot;) # add elevation back to the shapefiles watchPoints &lt;- bind_cols(watchPoints, elev_watch[,2]) names(watchPoints)[2] &lt;- &quot;elev&quot; gapPoints &lt;- bind_cols(gapPoints, elev_gap[,2]) names(gapPoints)[11] &lt;- &quot;elev&quot; potentialAM &lt;- bind_cols(potentialAM, elev_potentialAM[,2]) names(potentialAM)[9] &lt;- &quot;elev&quot; surveyDesign &lt;- bind_cols(surveyDesign, elev_surveyDesign[,2]) names(surveyDesign)[6] &lt;- &quot;elev&quot; ## editing the files to indicate what type of point it is watchPoints$type &lt;- &quot;watchPoint&quot; gapPoints$type &lt;- &quot;gapPoint&quot; potentialAM$type &lt;- &quot;potentialAM&quot; surveyDesign$type &lt;- &quot;surveyDesign&quot; 2.4 Interactive visualization of existing survey locations # visualization and editing vis &lt;- mapview(watchPoints, col.regions = &quot;black&quot;, legend = NULL) + mapview(gapPoints, col.regions = &quot;white&quot;) + mapview(potentialAM, col.regions = &quot;green&quot;) + mapview(surveyDesign, col.regions = &quot;brown&quot;) vis 2.5 Manually edit and choose new points for the study At this stage, we worked with Forestry agents from SVG to determine what points should be ultimately chosen for the monitoring of the St. Vincent’s parrot (If you have done this already, please skip and carry out other analysis). It has been commented out as this was already done on Nov 4th, 2024. ## Using functions from mapedit ## Note: the following lines of code can open an interactive html file that can be edited ## Please ensure that you save it by clicking DONE after editing the points # Please only run the following two lines of code if you haven&#39;t previously run it # visEdit &lt;- vis %&gt;% # editMap() 2.6 Convert the new survey locations to a dataframe # first, we will backtransform the elevation coordinate system alt.hills &lt;- project(alt.hills, &quot;epsg:4326&quot;) # new survey locations surveyLocs &lt;- data.frame(visEdit$finished, terra::extract(alt.hills, visEdit$finished)) names(surveyLocs)[5] &lt;- &quot;elevation_in_metres&quot; surveyLocs$elevation_in_metres &lt;- as.numeric(as.character(surveyLocs$elevation_in_metres)) ## load the survey point attributes taken down earlier ## during the workshop on Nov 4th, 2024 - a .csv was co-created with the forestry department staff to figure out audio recorder deployment locations ## this .csv was co-created while the interactive map was being edited. ## additional information on each location chosen interactively was added to separate columns ## in addition, a separate column was created to merge the survey location information (generated above interactively) with the attributes surveyAttr &lt;- read.csv(&quot;data/fieldwork/survey_pointAttributes.csv&quot;) ## add the survey attributes to the survey locations surveyLocs &lt;- right_join(surveyLocs, surveyAttr, by = c(&quot;ID&quot; = &quot;pointID_from_R&quot;)) ## Clean up the above file surveyLocs &lt;- surveyLocs[,c(3,5:9,11:13)] 2.7 Create shapefiles for fieldwork We will generate shapefiles that can be utilized for fieldwork. Please note that the surveyLocations.csv file generated below needs to be updated after fieldwork is carried out to include the updated locations when recorder deployment was carried out. surveyLocs &lt;- st_as_sf(surveyLocs) %&gt;% `st_crs&lt;-`(4326) # saving shapefiles for visualization/viewing in other softwares st_write(surveyLocs, &quot;data/spatial/preliminary_fieldwork_locations.shp&quot;, driver = &quot;ESRI Shapefile&quot;) # write out latitude and longitude for .csv surveyLocs &lt;- surveyLocs %&gt;% dplyr::mutate(longitude = sf::st_coordinates(.)[,1], latitude = sf::st_coordinates(.)[,2]) write.csv(surveyLocs, &quot;data/fieldwork/preliminary_fieldwork_locations.csv&quot;, row.names = F) 2.8 Saving an interactive file for collaborators # to save this to a file # if you would like to share this with collaborators/colleagues html_fl &lt;- tempfile(tmpdir = getwd(), fileext = &quot;interactive-visualization-preliminary-fieldwork-locations.html&quot;) ## load trails and range shapefiles before you take a screenshot trailA &lt;- st_read(&quot;data/spatial/trailA.shp&quot;) trailB &lt;- st_read(&quot;data/spatial/Trail_2.shp&quot;) trailC &lt;- st_read(&quot;data/spatial/Trails.shp&quot;) range &lt;- st_read(&quot;data/spatial/range_layer.shp&quot;) surveyMap &lt;- mapview(surveyLocs, col.regions = &quot;red&quot;) + mapview(trailA, col.regions = &quot;black&quot;) + mapview(trailB, col.regions = &quot;black&quot;) + mapview(trailC, col.regions = &quot;black&quot;) + mapview(range, col.regions = &quot;gray&quot;, alpha.regions = 0.01) # create standalone .html mapview::mapshot(surveyMap, url = html_fl) surveyMap 2.9 Create .gpx file for GPS Note that .gpx files are very sensitive to the .xml structure and the attributes need to be saved based on an appropriate format (For more information: https://www.topografix.com/GPX/1/1/#type_wptType). Once you create files in R, I would upload it in GPX Studio to make sure everything is accurate and save it again as a .gpx file to get the accurate format (https://gpx.studio/). # get only the attributes you need gpx &lt;- surveyLocs[,c(3,9)] # note that gpx files have specific metadata only, like name, elevation etc. names(gpx) &lt;- c(&quot;name&quot;,&quot;geometry&quot;) # sf and terra do not create .gpx files accurately: here, I have reverted to the use of rgdal # however, please note that rgdal was retired in 2023 and is no longer actively maintained # see git issue: https://github.com/r-spatial/sf/issues/2202 library(rgdal) gpx &lt;- as(gpx, &quot;Spatial&quot;) writeOGR(gpx, &quot;data/spatial/preliminary-fieldwork-locations-for-conversion.gpx&quot;, driver = &quot;GPX&quot;, layer = &quot;waypoints&quot;) # Now, please upload the above file in GPX Studio to make sure everything is accurate and save it again as a .gpx file to get the accurate format (https://gpx.studio/). Once fieldwork has been completed, please include the updated .gpx files and .shp files in the results/outputs folders. The .gpx file shared with Forestry staff is called preliminary-fieldwork-locations-for-gps.gpx and is located within the data/spatial/ folder. Once fieldwork was completed, the locations where recorders were deployed can be accessed from the data/fieldwork/recorder-deployment-retrival-info.csv or from data/acoustic-metadata.csv. "],["thresholding-birdnet-outputs.html", "Section 3 Thresholding BirdNET outputs 3.1 Load required libraries 3.2 Load selection tables from Raven Pro 3.3 Creating a subset of data using the thresholds above", " Section 3 Thresholding BirdNET outputs After having downloaded, pre-processed (following appropriate naming conventions and data management practices), creating a custom classifier for the St. Vincent’s Amazon and the Whistling Warbler, and validating those outputs using Raven Pro, we set a threshold above which the probability of detection of a species is at 90%, 95% or 99%. We can then use this threshold to filter all outputs from BirdNET and create a dataset of detections. 3.1 Load required libraries library(tidyverse) library(dplyr) library(stringr) library(ggplot2) library(data.table) library(extrafont) library(sf) library(raster) library(viridis) # for plotting library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(scico) 3.2 Load selection tables from Raven Pro Using Raven Pro, 200 random outputs from BirdNET with a range of confidence scores from 0.1 to 0.99 were manually validated and saved as a text file. We will load this text file and run a binomial glm to set a threshold. Please note that the validations were done for both the St. Vincent’s Amazon and the Whistling Warbler, but the classifier for the warbler is not doing a good job of retrieving detections of the species. We will be only analyzing the parrot data below and in other scripts. # read table table &lt;- read.table(&quot;results/birdNET-segments/stVincentAmazon/stVincentAmazon.Table.1.selections.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) # remove duplicates associated with waveform in the selection table table &lt;- table %&gt;% filter(View == &quot;Spectrogram 1&quot;) # extract confidence scores table$Score &lt;- as.numeric(substr(table$Begin.File, 1, 5)) # run model model &lt;- glm(Valid ~ Score, family = &quot;binomial&quot;, data = table) # create predictions prediction.range.conf &lt;- seq(0, 1, .001) predictions.conf &lt;- predict(model, list(Score = prediction.range.conf), type = &quot;r&quot;) # calculate thresholds threshold90 &lt;- (log(.90 / (1 - .90)) - model$coefficients[1]) / model$coefficients[2] threshold95 &lt;- (log(.95 / (1 - .95)) - model$coefficients[1]) / model$coefficients[2] threshold99 &lt;- (log(.99 / (1 - .99)) - model$coefficients[1]) / model$coefficients[2] # clean up labels for visualization threshold90_label &lt;- round(threshold90, 3) threshold95_label &lt;- round(threshold95, 3) threshold99_label &lt;- round(threshold99, 3) # set colors for the threshold lines viz_colors &lt;- magma(5) curve_color &lt;- viz_colors[1] threshold_colors &lt;- viz_colors[c(2, 3, 4)] # visualization fig_birdNET_threshold &lt;- ggplot(table, aes(x = Score, y = Valid)) + geom_point(alpha = 0.2) + geom_line( data = data.frame( x = prediction.range.conf, y = predictions.conf ), aes(x = x, y = y), color = curve_color, size = 1.2 ) + geom_vline(xintercept = threshold90, color = threshold_colors[1], size = 1) + geom_vline(xintercept = threshold95, color = threshold_colors[2], size = 1) + geom_vline(xintercept = threshold99, color = threshold_colors[3], size = 1) + # add threshold labels with offset annotate(&quot;text&quot;, x = pmin(pmax(threshold90, 0.05), 0.95) + 0.02, y = 0.5, label = paste(&quot;90%:&quot;, threshold90_label), color = threshold_colors[1], family = &quot;Century Gothic&quot;, angle = 90, hjust = 0.5, size = 3 ) + annotate(&quot;text&quot;, x = pmin(pmax(threshold95, 0.05), 0.95) + 0.02, y = 0.5, label = paste(&quot;95%:&quot;, threshold95_label), color = threshold_colors[2], family = &quot;Century Gothic&quot;, angle = 90, hjust = 0.5, size = 3 ) + annotate(&quot;text&quot;, x = pmin(pmax(threshold99, 0.05), 0.95) + 0.02, y = 0.5, label = paste(&quot;99%:&quot;, threshold99_label), color = threshold_colors[3], family = &quot;Century Gothic&quot;, angle = 90, hjust = 0.5, size = 3 ) + labs( title = &quot;&quot;, x = &quot;Confidence Score&quot;, y = &quot;Probability that a BirdNET prediction is correct&quot; ) + theme_bw() + theme( text = element_text(family = &quot;Century Gothic&quot;), plot.title = element_text(family = &quot;Century Gothic&quot;, size = 14), axis.title = element_text(family = &quot;Century Gothic&quot;, size = 12), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 10) ) ggsave(fig_birdNET_threshold, filename = &quot;figs/fig_birdNET_thresholds.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() Thresholds for 90, 95 and 99% probability that a BirdNET prediction is a true detection 3.3 Creating a subset of data using the thresholds above Based on the above plot, the threshold at which the probability of a true positive/detection is 90%, 95% or 99% is 0.204, 0.373 and 0.748, respectively. We will use the threshold at 95% to filter all detections for the St. Vincent’s Amazon. ## each prediction with an associated confidence score for a given species is stored within a .txt file within a series of sub-folders associated with different ranges and the way in which our data management and organization was carried out. ## we write functions to extract appropriate columns and filter by species # let&#39;s first write a function to process a single file process_file &lt;- function(file_path) { # read the text file data &lt;- read.csv(file_path, sep=&quot;\\t&quot;, stringsAsFactors=FALSE) # filter for StVincentAmazon filtered_data &lt;- data %&gt;% filter(Species.Code == &quot;stVincentAmazon&quot;) if(nrow(filtered_data) &gt; 0) { # extract file name without extension file_name &lt;- basename(file_path) file_name &lt;- str_replace(file_name, &quot;.BirdNET.selection.table&quot;, &quot;&quot;) # split the filename into components parts &lt;- str_split(file_name, &quot;_&quot;)[[1]] # create new columns from filename components filtered_data$range_code &lt;- parts[1] filtered_data$survey_point_number &lt;- parts[2] filtered_data$deployment_cycle &lt;- parts[3] filtered_data$date &lt;- parts[5] filtered_data$time &lt;- str_replace(parts[6], &quot;.txt&quot;, &quot;&quot;) filtered_data$site_protocol_code &lt;- paste(parts[1], parts[2], parts[3], sep=&quot;_&quot;) return(filtered_data) } return(NULL) } # function to recursively find all txt files find_txt_files &lt;- function(path) { list.files(path, pattern = &quot;\\\\.txt$&quot;, recursive = TRUE, full.names = TRUE) } # using the above functions to process data from the root directory main &lt;- function() { # set your root directory root_dir &lt;- &quot;results/birdNET-outputs/&quot; # replace with your actual path # get all txt files txt_files &lt;- find_txt_files(root_dir) # process all files and combine results results &lt;- list() for(file in txt_files) { result &lt;- process_file(file) if(!is.null(result)) { results[[length(results) + 1]] &lt;- result } } # combine all results into one dataframe final_df &lt;- bind_rows(results) return(final_df) } # run the main function result_data &lt;- main() ## we will now choose a subset of columns from the above object and filter predictions equal to or above the threshold for 95% probability of detections datSubset &lt;- result_data %&gt;% dplyr::select(Species.Code, Confidence, range_code, survey_point_number, deployment_cycle, date, time,site_protocol_code) %&gt;% filter(Confidence &gt;= threshold95) # rename columns and cleanup datSubset$common_name &lt;- &quot;St.Vincent Amazon&quot; datSubset$scientific_name &lt;- &quot;Amazona guildingii&quot; datSubset &lt;- datSubset[,-1] names(datSubset)[1] &lt;- &quot;confidence_score&quot; # include filename datSubset &lt;- datSubset %&gt;% mutate(filename = paste0(site_protocol_code,&quot;_&quot;, date,&quot;_&quot;,time)) # A summary of what we have so far # The total number of predictions from BirdNET: 97323 # At 99% probability: 64754 detections # At 95% probability: 79069 detections # At 90% probability: 87767 detections # exploratory visualization of distribution of confidence scores based on the threshold set fig_conf_score &lt;- ggplot(datSubset, aes(x = confidence_score, fill = survey_point_number)) + geom_histogram(binwidth = 0.1, position = &quot;dodge&quot;, color = &quot;black&quot;) + facet_wrap(~ survey_point_number, scales = &quot;free_y&quot;) + labs(title = &quot;Confidence Score Distribution by Site&quot;, x = &quot;Confidence Score&quot;, y = &quot;Count&quot;) + theme_bw() ggsave(fig_conf_score, filename = &quot;figs/fig_99p_distribution.png&quot;, width = 12, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() # for now, we work with a dataset of detections for a probability &gt;=95% # write to file write.csv(datSubset, &quot;results/datSubset.csv&quot;, row.names = FALSE) "],["spatial-analysis-of-birdnet-detections.html", "Section 4 Spatial analysis of BirdNET detections 4.1 Load necessary libraries 4.2 Load acoustic data and metadata 4.3 Visualizing sampling effort 4.4 How much data was recorded at each site? 4.5 Acoustic detections across days and months 4.6 Bubble map of acoustic detections", " Section 4 Spatial analysis of BirdNET detections In this script, we carry out spatial analyses of acoustic detections that were thresholded in the previous script. We work with a subset of detections that have a 95% probability of being a true positive. 4.1 Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(ggplot2) library(data.table) library(extrafont) library(sf) library(raster) library(stars) library(spatstat) library(mapview) # for plotting library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(scico) library(RColorBrewer) library(paletteer) 4.2 Load acoustic data and metadata metadata &lt;- read.csv(&quot;data/acoustic-metadata.csv&quot;) acoustic_data &lt;- read.csv(&quot;results/datSubset.csv&quot;) 4.3 Visualizing sampling effort How much acoustic data was collected? Using information on the first and last date of recorded acoustic data, we could visualize the total recorded days across months and sites. # ensure structure of dates in the metadata file is date metadata$first_file_date &lt;- ymd(metadata$first_file_date) metadata$last_file_date &lt;- ymd(metadata$last_file_date) ## create a factor of survey_point_number ordered by range_name metadata &lt;- metadata %&gt;% arrange(range_name, survey_point_number) %&gt;% mutate(survey_point_ordered = factor(survey_point_number, levels = unique(survey_point_number))) ## for the visualization range_info &lt;- metadata %&gt;% group_by(range_name) %&gt;% summarise(min_point = min(as.numeric(survey_point_ordered)), max_point = max(as.numeric(survey_point_ordered))) # create date breaks starting in November 2024 date_breaks &lt;- seq(as.Date(&quot;2024-11-01&quot;), max(metadata$last_file_date, na.rm = TRUE), by = &quot;month&quot;) # visualization fig_samplingEffort &lt;- metadata %&gt;% filter(!is.na(first_file_date)) %&gt;% ggplot(., aes(y = survey_point_ordered)) + geom_segment(aes(x = first_file_date, xend = last_file_date, yend = survey_point_ordered), color = &quot;#d95f02&quot;, size = 1) + # add vertical lines for range boundaries geom_hline(data = range_info, aes(yintercept = min_point - 0.5), color = &quot;grey20&quot;, size = 0.8) + geom_hline(data = range_info, aes(yintercept = max_point + 0.5), color = &quot;grey20&quot;, size = 0.8) + # add range names as annotations geom_text(data = range_info, aes(x = min(metadata$first_file_date, na.rm = TRUE), y = (min_point + max_point)/2, label = range_name), hjust = 0.5, color = &quot;grey10&quot;) + scale_x_date( limits = as.Date(c(&quot;2024-11-01&quot;, &quot;2025-07-01&quot;)), breaks = seq(as.Date(&quot;2024-11-01&quot;), as.Date(&quot;2025-07-01&quot;), by = &quot;month&quot;), date_labels = &quot;%b %Y&quot;, expand = expansion(mult = c(0.05, 0))) + labs(title = &quot;Sampling Effort&quot;, x = &quot;Acoustic Recording Dates&quot;, y = &quot;Survey Point Number&quot;) + theme_bw() + theme( text = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), plot.title = element_text( family = &quot;Century Gothic&quot;, size = 15, face = &quot;bold&quot; ), plot.subtitle = element_text( family = &quot;Century Gothic&quot;, size = 15, face = &quot;bold&quot;, color = &quot;#1b2838&quot; ), axis.title = element_text( family = &quot;Century Gothic&quot;, size = 18, face = &quot;bold&quot; ), legend.position = &quot;top&quot;, legend.title = element_blank(), legend.text = element_text(size = 10), axis.text.x = element_text(angle = 45, hjust = 1) ) ggsave(fig_samplingEffort, filename = &quot;figs/fig_sampling_effort.png&quot;, width = 14, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() Acoustic sampling effort shows that Windward South has the least gaps between deployment and had the most numbers of recording days, while Central range had a lot of gaps and very few recording days. Broadly, sampling was carried out from early Nov 2024 to the end of June 2025. 4.4 How much data was recorded at each site? Creating a bar plot with total number of days of recording at each site. Please refer to the metadata to ascertain the configuration and deployment details for the recorders. ## total number of recorded days recording_days &lt;- metadata %&gt;% rowwise() %&gt;% mutate( days = list(seq(first_file_date, last_file_date, by = &quot;day&quot;)) ) %&gt;% unnest(days) %&gt;% distinct(days, survey_point_number) %&gt;% group_by(survey_point_number) %&gt;% summarise( total_recording_days = n(), .groups = &#39;drop&#39; ) %&gt;% arrange(desc(total_recording_days)) %&gt;% left_join(metadata %&gt;% dplyr::select(survey_point_number, range_name) %&gt;% distinct(), by = &quot;survey_point_number&quot;) # calculate total days per range range_totals &lt;- recording_days %&gt;% group_by(range_name) %&gt;% summarise(total_days = sum(total_recording_days)) # visualization fig_totalRecordedDays &lt;- recording_days %&gt;% arrange(range_name, survey_point_number) %&gt;% mutate(survey_point_number = factor(survey_point_number, levels = unique(survey_point_number))) %&gt;% ggplot(., aes(x = survey_point_number, y = total_recording_days)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;#d95f02&quot;, alpha = 0.9) + geom_text(aes(label = total_recording_days), vjust = -0.5, size = 5, family = &quot;Century Gothic&quot;) + facet_grid(. ~ range_name, scales = &quot;free_x&quot;, space = &quot;free_x&quot;, labeller = labeller(range_name = function(x) paste0(x, &quot;\\nTotal recording days: &quot;, range_totals$total_days[range_totals$range_name == x]))) + theme_bw() + labs( x = &quot;\\nSurvey Point Number&quot;, y = &quot;Number of recording days\\n&quot;, title = &quot;&quot; ) + theme( axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 14), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = &quot;none&quot;, plot.title = element_text( family = &quot;Century Gothic&quot;, size = 16, face = &quot;bold&quot;, hjust = 0.5 ), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), strip.text = element_text( family = &quot;Century Gothic&quot;, size = 12, face = &quot;bold&quot; ), strip.background = element_rect(fill = &quot;grey95&quot;) ) ggsave(fig_totalRecordedDays, filename = &quot;figs/fig_total_days_audioRecording.png&quot;, width = 14, height = 8, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() Total number of audio recording days across sites and different ranges illustrate the Windward South had the most number of audio recording days followed by Leeward Range. 4.5 Acoustic detections across days and months # cumulative detections by site cumulative_detections &lt;- acoustic_data %&gt;% group_by(range_code, survey_point_number) %&gt;% summarise(cumulative_detections = n()) # here, observe that three sites have fewer than five detections and we filter these sites out for future analyses. These sites include PO27, PO35 and PO9. # group detections by day, range and site_name and filter out the above sites detections &lt;- acoustic_data %&gt;% group_by(range_code, survey_point_number, date) %&gt;% summarise(total_detections = n()) %&gt;% left_join(metadata %&gt;% dplyr::select(survey_point_number, range_name) %&gt;% distinct(), by = &quot;survey_point_number&quot;) # change date structure detections$date &lt;- ymd(detections$date) ## visualization fig_detections_days &lt;- detections %&gt;% ggplot(., aes(x = date, y = survey_point_number, fill = total_detections)) + geom_tile(color = &quot;black&quot;) + scale_fill_gradientn(colours = c(brewer.pal(9, &quot;Reds&quot;))) + scale_x_date(date_labels = &quot;%b %d %Y&quot;, date_breaks = &quot;14 days&quot;) + # faceting by range_name facet_grid(range_name ~ ., scales = &quot;free_y&quot;, space = &quot;free_y&quot;) + labs(title = &quot;Detections of St. Vincent Amazon across sites and days&quot;, x = &quot;Date of acoustic detection&quot;, y = &quot;Survey Point Number&quot;, fill = &quot;Acoustic detections&quot;) + theme_bw() + theme( axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 12), axis.text.x = element_text(angle = 45, hjust = 1, face = &quot;italic&quot;), legend.title = element_text(family = &quot;Century Gothic&quot;), legend.text = element_text(family = &quot;Century Gothic&quot;), strip.text = element_text( family = &quot;Century Gothic&quot;, size = 12, face = &quot;bold&quot; ), strip.background = element_rect(fill = &quot;grey95&quot;) ) ggsave(fig_detections_days, filename = &quot;figs/fig_detections_by_days.png&quot;, width = 14, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() Acoustic detections of the St. Vincent Amazon across the recording days for each range revealed a high number of detections across PO33 in the Leeward Range 4.6 Bubble map of acoustic detections ## load shapefiles of ranges st_vincent &lt;- st_read(&quot;data/spatial/range_layer.shp&quot;) st_vincent &lt;- st_transform(st_vincent, 4326) ## merge lat-long from metadata with the acoustic_data file acoustic_data &lt;- left_join(acoustic_data, metadata[,c(1,2,15:17)]) ## convert to an sf object acoustic_data &lt;- st_as_sf(acoustic_data, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = st_crs(st_vincent)) # get cumulative acoustic detections cumulative_detections &lt;- acoustic_data %&gt;% group_by(range_code, survey_point_number) %&gt;% summarise(cumulative_detections = n()) ## visualization fig_detections_bubbleMap &lt;- ggplot(data = st_vincent) + geom_sf(fill = NA, color = &quot;black&quot;) + scale_color_gradientn(colors = c(brewer.pal(5, &quot;Reds&quot;))) + geom_sf(data = cumulative_detections, aes(size = cumulative_detections, fill = cumulative_detections), shape = 21, alpha = 0.7, color = &quot;black&quot;) + geom_label_repel(data = cumulative_detections %&gt;% st_coordinates() %&gt;% as.data.frame() %&gt;% bind_cols(cumulative_detections %&gt;% st_drop_geometry()), aes(x = X, y = Y, label = survey_point_number), size = 2.5, family = &quot;Century Gothic&quot;, force = 1, label.padding = 0.15, box.padding = 0.5, point.padding = 0.5, min.segment.length = 0, seed = 42) + scale_size_continuous(range = c(3, 20)) + scale_fill_gradientn(colors = c(brewer.pal(5, &quot;Reds&quot;))) + labs(x = &#39;&#39;, y = &#39;&#39;, size = &#39;Acoustic detections&#39;, fill = &#39;Acoustic detections&#39;, title = &quot;Cumulative acoustic detections of the St. Vincent Amazon&quot;, subtitle = &quot;PO27, PO35 and PO9 had fewer than five detections&quot;) + theme_bw() + theme( plot.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;, hjust = 0.5 ), plot.subtitle = element_text( family = &quot;Century Gothic&quot;, size = 12, face = &quot;italic&quot;, hjust = 0.5 ), axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 12), legend.position = &quot;right&quot;, legend.title = element_text(family = &quot;Century Gothic&quot;, size = 10, face = &quot;bold&quot;), legend.text = element_text(family = &quot;Century Gothic&quot;, size = 10), panel.border = element_blank() ) ggsave(fig_detections_bubbleMap, filename = &quot;figs/fig_cumulativeDetections_bubbleMap.png&quot;, width = 9, height = 9, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() ## Weighting the bubble plot by sampling effort We will recreate the cumulative detections plot, but we will weight it by sampling effort. In our case, we will weight it by the total number of days spent recording. ## join recording days to cumulative detections cumulative_detections &lt;- left_join(cumulative_detections, recording_days) %&gt;% mutate(detections_by_effort = cumulative_detections/total_recording_days) ## visualization fig_detections_bubbleMap_by_effort &lt;- ggplot(data = st_vincent) + geom_sf(fill = NA, color = &quot;black&quot;) + scale_color_gradientn(colors = c(brewer.pal(5, &quot;Reds&quot;))) + geom_sf(data = cumulative_detections, aes(size = detections_by_effort, fill = detections_by_effort), shape = 21, alpha = 0.7, color = &quot;black&quot;) + geom_label_repel(data = cumulative_detections %&gt;% st_coordinates() %&gt;% as.data.frame() %&gt;% bind_cols(cumulative_detections %&gt;% st_drop_geometry()), aes(x = X, y = Y, label = survey_point_number), size = 2.5, family = &quot;Century Gothic&quot;, force = 1, label.padding = 0.15, box.padding = 0.5, point.padding = 0.5, min.segment.length = 0, seed = 42) + scale_size_continuous(range = c(3, 20)) + scale_fill_gradientn(colors = c(brewer.pal(5, &quot;Reds&quot;))) + labs(x = &#39;&#39;, y = &#39;&#39;, size = &#39;Acoustic detections&#39;, fill = &#39;Acoustic detections&#39;, title = &quot;Cumulative acoustic detections of the St. Vincent Amazon (controlling for sampling effort)&quot;, subtitle = &quot;PO27, PO35 and PO9 had fewer than five detections&quot;) + theme_bw() + theme( plot.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;, hjust = 0.5 ), plot.subtitle = element_text( family = &quot;Century Gothic&quot;, size = 12, face = &quot;italic&quot;, hjust = 0.5 ), axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 12), legend.position = &quot;right&quot;, legend.title = element_text(family = &quot;Century Gothic&quot;, size = 10, face = &quot;bold&quot;), legend.text = element_text(family = &quot;Century Gothic&quot;, size = 10), panel.border = element_blank() ) ggsave(fig_detections_bubbleMap_by_effort, filename = &quot;figs/fig_cumulativeDetections_by_effort_bubbleMap.png&quot;, width = 10, height = 10, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() A bubble map of cumulative acoustic detections of the St. Vincent Amazon across the entire duration of audio recording, while controlling for sampling effort in terms of numbers of recording days "],["spatiotemporal-analyses-of-birdnet-detections.html", "Section 5 Spatiotemporal analyses of BirdNET detections 5.1 Load necessary libraries 5.2 Load acoustic data and metadata 5.3 Creating a kernel density map of cumulative acoustic detections of the parrot across the island while controlling for sampling effort 5.4 Visualizing detections by time of day 5.5 Detections by time of day and month", " Section 5 Spatiotemporal analyses of BirdNET detections In this script, we carry out spatiotemporal analyses of acoustic detections. Specifically, we create kernel density maps of spatiotemporal variation in acoustic detections. 5.1 Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(ggplot2) library(data.table) library(extrafont) library(sf) library(raster) library(stars) library(spatstat) library(mapview) # for plotting library(scales) library(ggplot2) library(ggspatial) library(colorspace) library(scico) library(RColorBrewer) library(paletteer) 5.2 Load acoustic data and metadata metadata &lt;- read.csv(&quot;data/acoustic-metadata.csv&quot;) acoustic_data &lt;- read.csv(&quot;results/datSubset.csv&quot;) # ensure structure of dates in the metadata file is date metadata$first_file_date &lt;- ymd(metadata$first_file_date) metadata$last_file_date &lt;- ymd(metadata$last_file_date) ## create a factor of survey_point_number ordered by range_name metadata &lt;- metadata %&gt;% arrange(range_name, survey_point_number) %&gt;% mutate(survey_point_ordered = factor(survey_point_number, levels = unique(survey_point_number))) ## total number of recorded days recording_days &lt;- metadata %&gt;% rowwise() %&gt;% mutate( days = list(seq(first_file_date, last_file_date, by = &quot;day&quot;)) ) %&gt;% unnest(days) %&gt;% distinct(days, survey_point_number) %&gt;% group_by(survey_point_number) %&gt;% summarise( total_recording_days = n(), .groups = &#39;drop&#39; ) %&gt;% arrange(desc(total_recording_days)) %&gt;% left_join(metadata %&gt;% dplyr::select(survey_point_number, range_name) %&gt;% distinct(), by = &quot;survey_point_number&quot;) 5.3 Creating a kernel density map of cumulative acoustic detections of the parrot across the island while controlling for sampling effort ## load shapefiles of ranges st_vincent &lt;- st_read(&quot;data/spatial/range_layer.shp&quot;) st_vincent &lt;- st_transform(st_vincent, 4326) ## merge lat-long from metadata with the acoustic_data file acoustic_data &lt;- left_join(acoustic_data, metadata[,c(1,2,15:17)]) ## convert to an sf object acoustic_data &lt;- st_as_sf(acoustic_data, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = st_crs(st_vincent)) # get cumulative acoustic detections cumulative_detections &lt;- acoustic_data %&gt;% group_by(range_code, survey_point_number) %&gt;% summarise(cumulative_detections = n()) ## join recording days to cumulative detections cumulative_detections &lt;- left_join(cumulative_detections, recording_days) %&gt;% mutate(detections_by_effort = cumulative_detections/total_recording_days) # convert geographic coordinate system to projected coordinate system st_vincent &lt;- st_vincent %&gt;% st_transform(32620) cumulative_detections &lt;- cumulative_detections %&gt;% st_transform(32620) ## create spatial points object to create KDE plot ## for information on using as.ppp, refer to spatstat.geom package spp &lt;- as.ppp(st_coordinates(cumulative_detections), W = as.owin(st_vincent)) marks(spp) &lt;- round(cumulative_detections$detections_by_effort) ## create a stars object prior to plotting using sf &amp; ggplot2 weights &lt;- spp$marks # scale weights from 0 to 1 # this was done to ensure that the scale of comparison is the same across months range01 &lt;- function(x){(x-min(x))/(max(x)-min(x))} weights &lt;- range01(weights) density_obj &lt;- stars::st_as_stars(density(spp, dimyx = 70, weights = weights, edge = TRUE)) ## convert back to sf and change the CRS density_obj &lt;- st_as_sf(density_obj) st_crs(density_obj) &lt;- 32620 density_obj &lt;- st_intersection(density_obj, st_vincent) %&gt;% st_transform(4326) st_vincent &lt;- st_vincent %&gt;% st_transform(4326) cumulative_detections &lt;- cumulative_detections %&gt;% st_transform(4326) ## visualization fig_kdePlot &lt;- ggplot() + geom_sf(data = density_obj, aes(fill = v), color = NA) + geom_sf(data = st_vincent, fill = NA, color = &quot;white&quot;, linewidth = 0.25) + geom_sf(data = cumulative_detections, color = &quot;darkgray&quot;, shape = 21, fill = &quot;darkgray&quot;) + geom_text(data = cumulative_detections, aes(geometry = geometry, label = survey_point_number), stat = &quot;sf_coordinates&quot;, vjust = -0.9, hjust = 0.5, size = 2.5, family = &quot;Century Gothic&quot;, color = &quot;darkgray&quot;) + scale_fill_viridis_c(option = &quot;magma&quot;, name = &quot;Acoustic detections&quot;, trans = &quot;sqrt&quot;, breaks = c(min(density_obj$v), max(density_obj$v)), labels = c(&quot;Low&quot;, &quot;High&quot;))+ theme_bw() + labs(x = &#39;&#39;, y = &#39;&#39;, size = &#39;Acoustic detections&#39;, color = &#39;Acoustic detections&#39;, title = &quot;Cumulative acoustic detections of the St. Vincent Amazon&quot;, subtitle = &quot;Areas in black represent locations of no acoustic recording or very few detections&quot;) + theme( plot.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;, hjust = 0.5 ), plot.subtitle = element_text( family = &quot;Century Gothic&quot;, size = 10, face = &quot;italic&quot;, hjust = 0.5 ), axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 12), legend.position = &quot;right&quot;, legend.title = element_text(family = &quot;Century Gothic&quot;, size = 10, face = &quot;bold&quot;, margin = margin(b = 15)), legend.text = element_text(family = &quot;Century Gothic&quot;, size = 10), panel.border = element_blank() ) ggsave(fig_kdePlot, filename = &quot;figs/fig_kernelDensityMap_cumulativeDetections.png&quot;, width = 8, height = 7, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() ## Kernel density maps for different months of data Here, we replicate the above plot, but by creating a spatiotemporal series of plots for different months of sampling across the island. ## bin total recording days for each survey_point_number by different months recording_days &lt;- metadata %&gt;% rowwise() %&gt;% mutate( days = list(seq(first_file_date, last_file_date, by = &quot;day&quot;)) ) %&gt;% unnest(days) %&gt;% mutate( month = case_when( days &gt;= as.Date(&quot;2024-11-01&quot;) &amp; days &lt;= as.Date(&quot;2024-12-31&quot;) ~ &quot;Nov 2024 - Jan 2025&quot;, days &gt;= as.Date(&quot;2025-01-01&quot;) &amp; days &lt;= as.Date(&quot;2025-02-28&quot;) ~ &quot;Jan 2025 - Mar 2025&quot;, days &gt;= as.Date(&quot;2025-03-01&quot;) &amp; days &lt;= as.Date(&quot;2025-04-30&quot;) ~ &quot;Mar 2025 - May 2025&quot;, days &gt;= as.Date(&quot;2025-05-01&quot;) &amp; days &lt;= as.Date(&quot;2025-07-31&quot;) ~ &quot;May 2025 - July 2025&quot; ) ) %&gt;% distinct(days, survey_point_number, month) %&gt;% group_by(month, survey_point_number) %&gt;% summarise( recording_days = n(), .groups = &#39;drop&#39; ) %&gt;% mutate(month = factor(month, levels = c(&quot;Nov 2024 - Jan 2025&quot;, &quot;Jan 2025 - Mar 2025&quot;, &quot;Mar 2025 - May 2025&quot;, &quot;May 2025 - July 2025&quot;))) %&gt;% arrange(month, survey_point_number) # generate detections by month detections_by_month &lt;- acoustic_data %&gt;% mutate( date = as.Date(as.character(date), format = &quot;%Y%m%d&quot;), month = case_when( date &gt;= as.Date(&quot;2024-11-01&quot;) &amp; date &lt;= as.Date(&quot;2024-12-31&quot;) ~ &quot;Nov 2024 - Jan 2025&quot;, date &gt;= as.Date(&quot;2025-01-01&quot;) &amp; date &lt;= as.Date(&quot;2025-02-28&quot;) ~ &quot;Jan 2025 - Mar 2025&quot;, date &gt;= as.Date(&quot;2025-03-01&quot;) &amp; date &lt;= as.Date(&quot;2025-04-30&quot;) ~ &quot;Mar 2025 - May 2025&quot;, date &gt;= as.Date(&quot;2025-05-01&quot;) &amp; date &lt;= as.Date(&quot;2025-07-31&quot;) ~ &quot;May 2025 - July 2025&quot; ) ) %&gt;% group_by(month, survey_point_number) %&gt;% summarise( total_detections = n(), .groups = &#39;drop&#39; ) %&gt;% mutate(month = factor(month, levels = c(&quot;Nov 2024 - Jan 2025&quot;, &quot;Jan 2025 - Mar 2025&quot;, &quot;Mar 2025 - May 2025&quot;, &quot;May 2025 - July 2025&quot;))) %&gt;% arrange(month, survey_point_number) # merge dataframes monthly_detections &lt;- left_join(detections_by_month, recording_days, by = c(&quot;month&quot;, &quot;survey_point_number&quot;)) ## replicate the KDE plots so that we can generate them by the monthly time periods # create a list to store individual plots monthly_kde_plots &lt;- list() # define months in chronological order months &lt;- c(&quot;Nov 2024 - Jan 2025&quot;, &quot;Jan 2025 - Mar 2025&quot;, &quot;Mar 2025 - May 2025&quot;, &quot;May 2025 - July 2025&quot;) # creating a kde plot for each month for(current_month in months) { # subset data for current month month_data &lt;- monthly_detections %&gt;% filter(month == current_month) %&gt;% mutate(detections_by_effort = total_detections/recording_days) # transform coordinate systems st_vincent_proj &lt;- st_vincent %&gt;% st_transform(32620) month_data_proj &lt;- month_data %&gt;% st_transform(32620) # create spatial points object spp &lt;- as.ppp(st_coordinates(month_data_proj), W = as.owin(st_vincent_proj)) marks(spp) &lt;- round(month_data_proj$detections_by_effort) # create density object weights &lt;- spp$marks range01 &lt;- function(x){(x-min(x))/(max(x)-min(x))} weights &lt;- range01(weights) density_obj &lt;- stars::st_as_stars(density(spp, dimyx = 70, weights = weights, edge = TRUE)) # Convert back to sf and transform CRS density_obj &lt;- st_as_sf(density_obj) st_crs(density_obj) &lt;- 32620 density_obj &lt;- st_intersection(density_obj, st_vincent_proj) %&gt;% st_transform(4326) # create plot p &lt;- ggplot() + geom_sf(data = density_obj, aes(fill = v), color = NA) + geom_sf(data = st_vincent, fill = NA, color = &quot;white&quot;, linewidth = 0.25) + geom_sf(data = month_data, color = &quot;darkgray&quot;, shape = 21, fill = &quot;darkgray&quot;) + geom_text(data = month_data, aes(geometry = geometry, label = survey_point_number), stat = &quot;sf_coordinates&quot;, vjust = -0.9, hjust = 0.5, size = 2.5, family = &quot;Century Gothic&quot;, color = &quot;darkgray&quot;) + scale_fill_viridis_c(option = &quot;magma&quot;, name = &quot;Acoustic detections&quot;, trans = &quot;sqrt&quot;, breaks = c(min(density_obj$v), max(density_obj$v)), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + labs(x = &#39;&#39;, y = &#39;&#39;, size = &#39;Acoustic detections&#39;, color = &#39;Acoustic detections&#39;, title = paste(&quot;St. Vincent Amazon acoustic detections:&quot;, current_month), subtitle =&quot;Areas in black represent locations of no acoustic recording or very few detections&quot;) + theme( plot.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;, hjust = 0.5 ), plot.subtitle = element_text( family = &quot;Century Gothic&quot;, size = 10, face = &quot;italic&quot;, hjust = 0.5 ), axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 12), legend.position = &quot;right&quot;, legend.title = element_text(family = &quot;Century Gothic&quot;, size = 10, face = &quot;bold&quot;, margin = margin(b = 15)), legend.text = element_text(family = &quot;Century Gothic&quot;, size = 10), panel.border = element_blank() ) # save individual plot with numbered prefix for correct ordering month_number &lt;- which(months == current_month) filename &lt;- sprintf(&quot;figs/kde_%02d_%s.png&quot;, month_number, gsub(&quot; &quot;, &quot;_&quot;, current_month)) ggsave(filename = filename, plot = p, width = 8, height = 7, dpi = 300) # store plot in list monthly_kde_plots[[current_month]] &lt;- p } # create GIF using magick package library(magick) # list all the PNG files in correct order img_files &lt;- list.files(&quot;figs&quot;, pattern = &quot;kde_.*\\\\.png&quot;, full.names = TRUE) img_files &lt;- sort(img_files) # read images imgs &lt;- image_read(img_files) # create GIF gif &lt;- image_animate(imgs, fps = 0.5) # save GIF image_write(gif, &quot;figs/monthly_kde_animation.gif&quot;) Kernel density maps of species acoustic detections across different months for the entire time period 5.4 Visualizing detections by time of day Here, we visualize by time of day while weighting for the total recording hours for that time period. recording_hours_by_timeframe &lt;- metadata %&gt;% # first handle the dates and times mutate( # convert HHMMSS to decimal hours first_hour = as.numeric(substr(sprintf(&quot;%06d&quot;, first_file_time), 1, 2)), first_minute = as.numeric(substr(sprintf(&quot;%06d&quot;, first_file_time), 3, 4)) / 60, last_hour = as.numeric(substr(sprintf(&quot;%06d&quot;, last_file_time), 1, 2)), last_minute = as.numeric(substr(sprintf(&quot;%06d&quot;, last_file_time), 3, 4)) / 60, first_decimal_time = first_hour + first_minute, last_decimal_time = last_hour + last_minute, days = as.numeric(last_file_date - first_file_date), sampling_periods = str_split(aru_sampling_times, &quot;; &quot;) ) %&gt;% unnest(sampling_periods) %&gt;% separate(sampling_periods, into = c(&quot;start_time&quot;, &quot;end_time&quot;), sep = &quot;-&quot;) %&gt;% # convert scheduled periods to decimal hours mutate( period_start_hour = as.numeric(substr(start_time, 1, 2)), period_end_hour = as.numeric(substr(end_time, 1, 2)) ) %&gt;% # generate hourly time frames rowwise() %&gt;% mutate( hours = list(seq(period_start_hour, period_end_hour - 1)) ) %&gt;% unnest(hours) %&gt;% # create time frame labels and calculate hours for each frame mutate( time_frame = sprintf(&quot;%02d:00-%02d:00&quot;, hours, hours + 1), # calculate hours for first day first_day_hours = case_when( hours &gt;= first_hour ~ 1, # full hour if after start hours + 1 &gt; first_decimal_time ~ (hours + 1) - first_decimal_time, # partial hour at start TRUE ~ 0 ), # calculate hours for last day last_day_hours = case_when( hours + 1 &lt;= last_hour ~ 1, # full hour if before end hours &lt; last_decimal_time ~ last_decimal_time - hours, # partial hour at end TRUE ~ 0 ), # calculate hours for middle days using case_when middle_days_hours = case_when( days &gt; 0 ~ 1, TRUE ~ 0 ) ) %&gt;% group_by(survey_point_number, time_frame) %&gt;% summarise( total_hours = sum(first_day_hours + (middle_days_hours * (days - 1)) + # subtract 1 to not double count first/last days last_day_hours), .groups = &#39;drop&#39; ) %&gt;% # add range information left_join(metadata %&gt;% dplyr::select(survey_point_number, range_name) %&gt;% distinct(), by = &quot;survey_point_number&quot;) %&gt;% # order time frames chronologically mutate( time_frame = factor(time_frame, levels = sort(unique(time_frame))) ) %&gt;% arrange(survey_point_number, time_frame) # get detections by timeframe detections_by_timeframe &lt;- acoustic_data %&gt;% mutate( hour = as.numeric(substr(sprintf(&quot;%06d&quot;, time), 1, 2)), time_frame = sprintf(&quot;%02d:00-%02d:00&quot;, hour, hour + 1) ) %&gt;% group_by(range_code, survey_point_number, time_frame) %&gt;% summarise(total_detections = n(), .groups = &#39;drop&#39;) %&gt;% mutate( time_frame = factor(time_frame, levels = sort(unique(time_frame))) ) # join with recording hours and calculate detections per hour detections_per_hour &lt;- detections_by_timeframe %&gt;% left_join(recording_hours_by_timeframe, by = c(&quot;survey_point_number&quot;, &quot;time_frame&quot;)) %&gt;% mutate( detections_per_hour = total_detections / total_hours ) # create the sequence of time frames in groups morning_times &lt;- c(&quot;04:00-05:00&quot;, &quot;05:00-06:00&quot;, &quot;06:00-07:00&quot;) midday_times &lt;- c(&quot;10:00-11:00&quot;, &quot;11:00-12:00&quot;, &quot;12:00-13:00&quot;, &quot;13:00-14:00&quot;) evening_times &lt;- c(&quot;17:00-18:00&quot;, &quot;18:00-19:00&quot;) # combine all times but keep track of groups time_sequence &lt;- c(morning_times, midday_times, evening_times) # recordering detections_per_hour dataframe detections_per_hour_ordered &lt;- detections_per_hour %&gt;% # reorder time_frame factor levels mutate( time_frame = factor(time_frame, levels = time_sequence), # create a grouping variable time_group = case_when( time_frame %in% morning_times ~ 1, time_frame %in% midday_times ~ 2, time_frame %in% evening_times ~ 3 ) ) ## visualization fig_detections_per_hour &lt;- detections_per_hour_ordered %&gt;% ggplot(aes(x = time_frame, y = survey_point_number, fill = detections_per_hour)) + geom_tile(color = &quot;black&quot;) + scale_fill_gradientn(colours = c(brewer.pal(9, &quot;Reds&quot;))) + facet_grid(range_name ~ ., scales = &quot;free_y&quot;, space = &quot;free_y&quot;) + labs(title = &quot;Detections per hour of St. Vincent Amazon across sites&quot;, x = &quot;Time of day&quot;, y = &quot;Survey Point Number&quot;, fill = &quot;Detections\\nper hour&quot;, subtitle = &quot;Analysis controls for total recording hours&quot;) + theme_bw() + theme( axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), plot.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(family = &quot;Century Gothic&quot;, face = &quot;italic&quot;, size = 10), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 12), axis.text.x = element_text(angle = 45, hjust = 1), legend.title = element_text(family = &quot;Century Gothic&quot;), legend.text = element_text(family = &quot;Century Gothic&quot;), strip.text = element_text( family = &quot;Century Gothic&quot;, size = 12, face = &quot;bold&quot; ), strip.background = element_rect(fill = &quot;grey95&quot;), panel.spacing.x = unit(1, &quot;lines&quot;) ) + scale_x_discrete( breaks = time_sequence, expand = expansion(mult = 0.05), position = &quot;bottom&quot;, drop = FALSE, limits = c(morning_times, &quot;&quot;, midday_times, &quot;&quot;, evening_times) ) ggsave(fig_detections_per_hour, filename = &quot;figs/fig_detections_per_hour.png&quot;, width = 14, height = 9, device = png(), units = &quot;in&quot;, dpi = 300) dev.off() Detections by hour across sites for the entire sampling period, while controlling for total recording hours 5.5 Detections by time of day and month # calculate recording hours by month and timeframe recording_hours_by_month_timeframe &lt;- metadata %&gt;% # create date sequence for each point rowwise() %&gt;% mutate( dates = list(seq(first_file_date, last_file_date, by = &quot;day&quot;)) ) %&gt;% unnest(dates) %&gt;% # assign monthly periods mutate( month = case_when( dates &gt;= as.Date(&quot;2024-11-01&quot;) &amp; dates &lt;= as.Date(&quot;2024-12-31&quot;) ~ &quot;Nov 2024 - Jan 2025&quot;, dates &gt;= as.Date(&quot;2025-01-01&quot;) &amp; dates &lt;= as.Date(&quot;2025-02-28&quot;) ~ &quot;Jan 2025 - Mar 2025&quot;, dates &gt;= as.Date(&quot;2025-03-01&quot;) &amp; dates &lt;= as.Date(&quot;2025-04-30&quot;) ~ &quot;Mar 2025 - May 2025&quot;, dates &gt;= as.Date(&quot;2025-05-01&quot;) &amp; dates &lt;= as.Date(&quot;2025-07-31&quot;) ~ &quot;May 2025 - July 2025&quot; ), # convert first and last file times to decimal hours first_hour = as.numeric(substr(sprintf(&quot;%06d&quot;, first_file_time), 1, 2)), first_minute = as.numeric(substr(sprintf(&quot;%06d&quot;, first_file_time), 3, 4)) / 60, last_hour = as.numeric(substr(sprintf(&quot;%06d&quot;, last_file_time), 1, 2)), last_minute = as.numeric(substr(sprintf(&quot;%06d&quot;, last_file_time), 3, 4)) / 60, first_decimal_time = first_hour + first_minute, last_decimal_time = last_hour + last_minute ) %&gt;% # split sampling periods separate_rows(aru_sampling_times, sep = &quot;; &quot;) %&gt;% separate(aru_sampling_times, into = c(&quot;start_time&quot;, &quot;end_time&quot;), sep = &quot;-&quot;) %&gt;% mutate( start_hour = as.numeric(substr(start_time, 1, 2)), end_hour = as.numeric(substr(end_time, 1, 2)) ) %&gt;% rowwise() %&gt;% mutate( hours = list(seq(start_hour, end_hour - 1)) ) %&gt;% unnest(hours) %&gt;% mutate( time_frame = sprintf(&quot;%02d:00-%02d:00&quot;, hours, hours + 1), # calculate daily hours daily_hours = case_when( # first day dates == first_file_date &amp; hours &lt; first_decimal_time ~ 0, dates == first_file_date &amp; hours &gt;= first_decimal_time ~ 1, # last day dates == last_file_date &amp; hours &gt;= last_decimal_time ~ 0, dates == last_file_date &amp; hours &lt; last_decimal_time ~ 1, # middle days - full hours within sampling period TRUE ~ 1 ) ) %&gt;% # sum up actual hours group_by(survey_point_number, month, time_frame) %&gt;% summarise( total_hours = sum(daily_hours), .groups = &#39;drop&#39; ) # calculate detections by month and timeframe detections_by_month_timeframe &lt;- acoustic_data %&gt;% mutate( date = as.Date(as.character(date), format = &quot;%Y%m%d&quot;), hour = as.numeric(substr(sprintf(&quot;%06d&quot;, time), 1, 2)), time_frame = sprintf(&quot;%02d:00-%02d:00&quot;, hour, hour + 1), month = case_when( date &gt;= as.Date(&quot;2024-11-01&quot;) &amp; date &lt;= as.Date(&quot;2024-12-31&quot;) ~ &quot;Nov 2024 - Jan 2025&quot;, date &gt;= as.Date(&quot;2025-01-01&quot;) &amp; date &lt;= as.Date(&quot;2025-02-28&quot;) ~ &quot;Jan 2025 - Mar 2025&quot;, date &gt;= as.Date(&quot;2025-03-01&quot;) &amp; date &lt;= as.Date(&quot;2025-04-30&quot;) ~ &quot;Mar 2025 - May 2025&quot;, date &gt;= as.Date(&quot;2025-05-01&quot;) &amp; date &lt;= as.Date(&quot;2025-07-31&quot;) ~ &quot;May 2025 - July 2025&quot; ) ) %&gt;% group_by(survey_point_number, month, time_frame) %&gt;% summarise( total_detections = n(), .groups = &#39;drop&#39; ) # create final dataset detections_per_hour_by_month &lt;- detections_by_month_timeframe %&gt;% # join with recording hours inner_join(recording_hours_by_month_timeframe, by = c(&quot;survey_point_number&quot;, &quot;month&quot;, &quot;time_frame&quot;)) %&gt;% # add coordinates left_join(metadata %&gt;% dplyr::select(survey_point_number, longitude, latitude) %&gt;% distinct(), by = &quot;survey_point_number&quot;) %&gt;% # calculate detections per hour mutate( detections_per_hour = total_detections / total_hours ) %&gt;% # convert to sf object st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% # factor the month variable for proper ordering mutate(month = factor(month, levels = c(&quot;Nov 2024 - Jan 2025&quot;, &quot;Jan 2025 - Mar 2025&quot;, &quot;Mar 2025 - May 2025&quot;, &quot;May 2025 - July 2025&quot;))) # create a list to store individual plots monthly_hourly_kde_plots &lt;- list() # define months and time periods months &lt;- c(&quot;Nov 2024 - Jan 2025&quot;, &quot;Jan 2025 - Mar 2025&quot;, &quot;Mar 2025 - May 2025&quot;, &quot;May 2025 - July 2025&quot;) # get unique time frames from your data time_frames &lt;- sort(unique(detections_per_hour_by_month$time_frame)) # creating a kde plot for each month and time frame combination for(current_month in months) { for(current_time in time_frames) { # subset data for current month and time frame period_data &lt;- detections_per_hour_by_month %&gt;% filter(month == current_month, time_frame == current_time, !is.na(detections_per_hour)) # Remove NA values # only proceed if we have enough data if(nrow(period_data) &gt;= 3) { # KDE needs at least 3 points # transform coordinate systems st_vincent_proj &lt;- st_vincent %&gt;% st_transform(32620) period_data_proj &lt;- period_data %&gt;% st_transform(32620) # create spatial points object spp &lt;- as.ppp(st_coordinates(period_data_proj), W = as.owin(st_vincent_proj)) # handle weights weights &lt;- period_data_proj$detections_per_hour if(all(weights == 0)) { # skip this period if all weights are zero next } # normalize weights to avoid extremely small values range01 &lt;- function(x){(x-min(x))/(max(x)-min(x))} weights &lt;- range01(weights) # check for NA/NaN in weights if(any(is.na(weights))) { print(paste(&quot;NA weights found in&quot;, current_month, current_time)) next } # create density object density_obj &lt;- stars::st_as_stars(density(spp, dimyx = 70, weights = weights, edge = TRUE)) # Convert back to sf and transform CRS density_obj &lt;- st_as_sf(density_obj) st_crs(density_obj) &lt;- 32620 density_obj &lt;- st_intersection(density_obj, st_vincent_proj) %&gt;% st_transform(4326) # create plot p &lt;- ggplot() + geom_sf(data = density_obj, aes(fill = v), color = NA) + geom_sf(data = st_vincent, fill = NA, color = &quot;white&quot;, linewidth = 0.25) + geom_sf(data = period_data, color = &quot;darkgray&quot;, shape = 21, fill = &quot;darkgray&quot;) + geom_text(data = period_data, aes(geometry = geometry, label = survey_point_number), stat = &quot;sf_coordinates&quot;, vjust = -0.9, hjust = 0.5, size = 2.5, family = &quot;Century Gothic&quot;, color = &quot;darkgray&quot;) + scale_fill_viridis_c(option = &quot;magma&quot;, name = &quot;Detections per hour&quot;, trans = &quot;sqrt&quot;, breaks = c(min(density_obj$v), max(density_obj$v)), labels = c(&quot;Low&quot;, &quot;High&quot;)) + theme_bw() + labs(x = &#39;&#39;, y = &#39;&#39;, size = &#39;Detections per hour&#39;, color = &#39;Detections per hour&#39;, title = paste(&quot;St. Vincent Amazon acoustic detections:&quot;, current_month), subtitle = paste(&quot;Time period:&quot;, current_time)) + theme( plot.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;, hjust = 0.5 ), plot.subtitle = element_text( family = &quot;Century Gothic&quot;, size = 10, face = &quot;italic&quot;, hjust = 0.5 ), axis.title = element_text( family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot; ), axis.text = element_text(family = &quot;Century Gothic&quot;, size = 12), legend.position = &quot;right&quot;, legend.title = element_text( family = &quot;Century Gothic&quot;, size = 10, face = &quot;bold&quot;, margin = margin(b = 15) ), legend.text = element_text(family = &quot;Century Gothic&quot;, size = 10), panel.border = element_blank() ) # save individual plot time_number &lt;- which(time_frames == current_time) month_number &lt;- which(months == current_month) filename &lt;- sprintf(&quot;figs/kde_%02d_%02d_%s_%s.png&quot;, month_number, time_number, gsub(&quot; &quot;, &quot;_&quot;, current_month), gsub(&quot;:&quot;, &quot;&quot;, current_time)) ggsave(filename = filename, plot = p, width = 8, height = 7, dpi = 300) # store plot in list monthly_hourly_kde_plots[[paste(current_month, current_time)]] &lt;- p } } } # create separate GIFs for each month library(magick) for(current_month in months) { # list all the PNG files for current month in correct order month_pattern &lt;- paste0(&quot;kde_.*&quot;, gsub(&quot; &quot;, &quot;_&quot;, current_month)) img_files &lt;- list.files(&quot;figs&quot;, pattern = month_pattern, full.names = TRUE) img_files &lt;- sort(img_files) # Only proceed if we have files if(length(img_files) &gt; 0) { # read images imgs &lt;- image_read(img_files) # create GIF gif &lt;- image_animate(imgs, fps = 0.5) # save GIF gif_filename &lt;- paste0(&quot;figs/monthly_kde_animation_by_time_of_day&quot;, gsub(&quot; &quot;, &quot;_&quot;, current_month), &quot;.gif&quot;) image_write(gif, gif_filename) } } Spatiotemporal clustering of acoustic detections between Nov 2024 and Jan 2025 Spatiotemporal clustering of acoustic detections between Jan 2025 and Mar 2025 Spatiotemporal clustering of acoustic detections between Mar 2025 and May 2025 Spatiotemporal clustering of acoustic detections between May 2025 and July 2025 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
